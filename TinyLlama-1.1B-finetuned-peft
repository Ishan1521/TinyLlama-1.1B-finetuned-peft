{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30887,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-02-18T19:41:32.012533Z","iopub.execute_input":"2025-02-18T19:41:32.012845Z","iopub.status.idle":"2025-02-18T19:41:32.978786Z","shell.execute_reply.started":"2025-02-18T19:41:32.012813Z","shell.execute_reply":"2025-02-18T19:41:32.977946Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"!pip install --upgrade pip setuptools\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-18T19:41:37.036322Z","iopub.execute_input":"2025-02-18T19:41:37.036671Z","iopub.status.idle":"2025-02-18T19:41:54.969267Z","shell.execute_reply.started":"2025-02-18T19:41:37.036640Z","shell.execute_reply":"2025-02-18T19:41:54.968214Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: pip in /usr/local/lib/python3.10/dist-packages (24.1.2)\nCollecting pip\n  Downloading pip-25.0.1-py3-none-any.whl.metadata (3.7 kB)\nRequirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (75.1.0)\nCollecting setuptools\n  Downloading setuptools-75.8.0-py3-none-any.whl.metadata (6.7 kB)\nDownloading pip-25.0.1-py3-none-any.whl (1.8 MB)\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m31.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hDownloading setuptools-75.8.0-py3-none-any.whl (1.2 MB)\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m55.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: setuptools, pip\n  Attempting uninstall: setuptools\n    Found existing installation: setuptools 75.1.0\n    Uninstalling setuptools-75.1.0:\n      Successfully uninstalled setuptools-75.1.0\n  Attempting uninstall: pip\n    Found existing installation: pip 24.1.2\n    Uninstalling pip-24.1.2:\n      Successfully uninstalled pip-24.1.2\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\npandas-gbq 0.25.0 requires google-api-core<3.0.0dev,>=2.10.2, but you have google-api-core 1.34.1 which is incompatible.\ntensorflow-decision-forests 1.10.0 requires tensorflow==2.17.0, but you have tensorflow 2.17.1 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed pip-25.0.1 setuptools-75.8.0\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install --no-cache-dir bitsandbytes\n!pip install transformers peft accelerate datasets scipy einops evaluate trl rouge_score\n!pip show rouge_score","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-18T19:43:15.882054Z","iopub.execute_input":"2025-02-18T19:43:15.882366Z","iopub.status.idle":"2025-02-18T19:43:25.678442Z","shell.execute_reply.started":"2025-02-18T19:43:15.882341Z","shell.execute_reply":"2025-02-18T19:43:25.677626Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: bitsandbytes in /usr/local/lib/python3.10/dist-packages (0.45.2)\nRequirement already satisfied: torch<3,>=2.0 in /usr/local/lib/python3.10/dist-packages (from bitsandbytes) (2.5.1+cu121)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from bitsandbytes) (1.26.4)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->bitsandbytes) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->bitsandbytes) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->bitsandbytes) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->bitsandbytes) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->bitsandbytes) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->bitsandbytes) (2.4.1)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.17.0)\nRequirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch<3,>=2.0->bitsandbytes) (4.12.2)\nRequirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.1.4)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch<3,>=2.0->bitsandbytes) (2024.9.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch<3,>=2.0->bitsandbytes) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch<3,>=2.0->bitsandbytes) (1.3.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch<3,>=2.0->bitsandbytes) (3.0.2)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->bitsandbytes) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->bitsandbytes) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->bitsandbytes) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy>=1.17->bitsandbytes) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy>=1.17->bitsandbytes) (2024.2.0)\nRequirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.47.0)\nRequirement already satisfied: peft in /usr/local/lib/python3.10/dist-packages (0.14.0)\nRequirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (1.2.1)\nRequirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (3.2.0)\nRequirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (1.13.1)\nRequirement already satisfied: einops in /usr/local/lib/python3.10/dist-packages (0.8.0)\nRequirement already satisfied: evaluate in /usr/local/lib/python3.10/dist-packages (0.4.3)\nRequirement already satisfied: trl in /usr/local/lib/python3.10/dist-packages (0.15.1)\nRequirement already satisfied: rouge_score in /usr/local/lib/python3.10/dist-packages (0.1.2)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.17.0)\nRequirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.28.1)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.2)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.11.6)\nRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\nRequirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.21.0)\nRequirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\nRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.67.1)\nRequirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from peft) (5.9.5)\nRequirement already satisfied: torch>=1.13.0 in /usr/local/lib/python3.10/dist-packages (from peft) (2.5.1+cu121)\nRequirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (19.0.0)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\nRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.2.3)\nRequirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.5.0)\nRequirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\nRequirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets) (2024.9.0)\nRequirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.11.11)\nRequirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from trl) (13.9.4)\nRequirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from rouge_score) (1.4.0)\nRequirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from rouge_score) (3.2.4)\nRequirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from rouge_score) (1.17.0)\nRequirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.4)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.2)\nRequirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (5.0.1)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (25.1.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.5.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (0.2.1)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.18.3)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (4.12.2)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (2.4.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2025.1.31)\nRequirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (3.1.4)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.13.0->peft) (1.3.0)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2025.1)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2025.1)\nRequirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->trl) (3.0.0)\nRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->trl) (2.19.1)\nRequirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->trl) (0.1.2)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.13.0->peft) (3.0.2)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->transformers) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->transformers) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->transformers) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy>=1.17->transformers) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy>=1.17->transformers) (2024.2.0)\nName: rouge_score\nVersion: 0.1.2\nSummary: Pure python implementation of ROUGE-1.5.5.\nHome-page: https://github.com/google-research/google-research/tree/master/rouge\nAuthor: Google LLC\nAuthor-email: rouge-opensource@google.com\nLicense: \nLocation: /usr/local/lib/python3.10/dist-packages\nRequires: absl-py, nltk, numpy, six\nRequired-by: \n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"import bitsandbytes\nimport transformers\nimport peft\nimport accelerate\nimport datasets\nimport scipy\nimport einops\nimport evaluate\nimport trl\nimport rouge_score\nimport pkg_resources\n\nprint(\"BitsAndBytes version:\", bitsandbytes.__version__)\nprint(\"Transformers version:\", transformers.__version__)\nprint(\"PEFT version:\", peft.__version__)\nprint(\"Accelerate version:\", accelerate.__version__)\nprint(\"Datasets version:\", datasets.__version__)\nprint(\"Scipy version:\", scipy.__version__)\nprint(\"Einops version:\", einops.__version__)\nprint(\"Evaluate version:\", evaluate.__version__)\nprint(\"TRL version:\", trl.__version__)\nprint(\"Rouge Score version:\", pkg_resources.get_distribution(\"rouge_score\").version)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-18T19:43:30.361273Z","iopub.execute_input":"2025-02-18T19:43:30.361627Z","iopub.status.idle":"2025-02-18T19:43:53.050993Z","shell.execute_reply.started":"2025-02-18T19:43:30.361596Z","shell.execute_reply":"2025-02-18T19:43:53.050154Z"}},"outputs":[{"name":"stdout","text":"BitsAndBytes version: 0.45.2\nTransformers version: 4.47.0\nPEFT version: 0.14.0\nAccelerate version: 1.2.1\nDatasets version: 3.2.0\nScipy version: 1.13.1\nEinops version: 0.8.0\nEvaluate version: 0.4.3\nTRL version: 0.15.1\nRouge Score version: 0.1.2\n","output_type":"stream"},{"name":"stderr","text":"<ipython-input-5-d718403d2285>:11: DeprecationWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html\n  import pkg_resources\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"import os\n# disable Weights and Biases\nos.environ['WANDB_DISABLED']=\"true\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-18T19:44:03.374532Z","iopub.execute_input":"2025-02-18T19:44:03.375141Z","iopub.status.idle":"2025-02-18T19:44:03.378888Z","shell.execute_reply.started":"2025-02-18T19:44:03.375113Z","shell.execute_reply":"2025-02-18T19:44:03.377980Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"import torch\ntorch.cuda.empty_cache()  # ‚úÖ Free unused memory\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport transformers\nfrom peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\nfrom datasets import load_dataset\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, Trainer, DataCollatorForLanguageModeling\nfrom transformers import BitsAndBytesConfig\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-18T19:44:38.683897Z","iopub.execute_input":"2025-02-18T19:44:38.684177Z","iopub.status.idle":"2025-02-18T19:44:39.704464Z","shell.execute_reply.started":"2025-02-18T19:44:38.684156Z","shell.execute_reply":"2025-02-18T19:44:39.703838Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"# ‚úÖ Set device\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-18T19:44:48.289785Z","iopub.execute_input":"2025-02-18T19:44:48.290076Z","iopub.status.idle":"2025-02-18T19:44:48.294015Z","shell.execute_reply.started":"2025-02-18T19:44:48.290053Z","shell.execute_reply":"2025-02-18T19:44:48.293055Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"# ‚úÖ Load dataset\nhuggingface_dataset_name = \"neil-code/dialogsum-test\"\ndataset = load_dataset(huggingface_dataset_name)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-18T19:44:56.999694Z","iopub.execute_input":"2025-02-18T19:44:56.999983Z","iopub.status.idle":"2025-02-18T19:44:59.588108Z","shell.execute_reply.started":"2025-02-18T19:44:56.999962Z","shell.execute_reply":"2025-02-18T19:44:59.587503Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/4.56k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d52f876181da492da6bd6da306678ad7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"train.csv:   0%|          | 0.00/1.81M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dccbf61229824dbdbccee91d535d1e96"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"validation.csv:   0%|          | 0.00/441k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4a96dba7aca648a4b47e51447cfd2de3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"test.csv:   0%|          | 0.00/447k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"55ad55ad71ea41999e11577140bae59e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/1999 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e1f028ccf1f14c389add0e6961f3054d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating validation split:   0%|          | 0/499 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"851616fda91f4e18a2331504cd43738e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split:   0%|          | 0/499 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"05d93177710342098d3fdd43fd071e60"}},"metadata":{}}],"execution_count":9},{"cell_type":"code","source":"\n# ‚úÖ Free memory before loading model\ntorch.cuda.empty_cache()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-18T19:45:15.034001Z","iopub.execute_input":"2025-02-18T19:45:15.034288Z","iopub.status.idle":"2025-02-18T19:45:15.038074Z","shell.execute_reply.started":"2025-02-18T19:45:15.034267Z","shell.execute_reply":"2025-02-18T19:45:15.037330Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"# ‚úÖ Load tokenizer\nmodel_name = \"TinyLlama/TinyLlama-1.1B-chat-v1.0\"\ntokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-18T19:45:25.830189Z","iopub.execute_input":"2025-02-18T19:45:25.830515Z","iopub.status.idle":"2025-02-18T19:45:27.487814Z","shell.execute_reply.started":"2025-02-18T19:45:25.830491Z","shell.execute_reply":"2025-02-18T19:45:27.487088Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/1.29k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e6ea837cbe9847a88dee349fbab80b79"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"24cf615bb05c46088872f6de5eb496e1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.84M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"80cf21bce2eb417ab9b16e18c244e73a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/551 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2678f998d0cd48788b7e4717f279f61d"}},"metadata":{}}],"execution_count":11},{"cell_type":"code","source":"\n# ‚úÖ Load model with 4-bit quantization\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True, \n    bnb_4bit_quant_type=\"nf4\", \n    bnb_4bit_compute_dtype=torch.bfloat16\n)\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    quantization_config=bnb_config,\n    device_map=\"auto\",\n    trust_remote_code=True\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-18T19:45:39.012650Z","iopub.execute_input":"2025-02-18T19:45:39.012941Z","iopub.status.idle":"2025-02-18T19:46:34.462988Z","shell.execute_reply.started":"2025-02-18T19:45:39.012919Z","shell.execute_reply":"2025-02-18T19:46:34.462119Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/608 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"41e70d8cba2e494ca3df7e100385c182"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/2.20G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"673137a00ece4592a402ab39f1b67c38"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"662be8589b8f4432b7621a946204f0e2"}},"metadata":{}}],"execution_count":12},{"cell_type":"code","source":"\n# ‚úÖ LoRA Configuration\nconfig = LoraConfig(\n    r=16,  \n    lora_alpha=16,\n    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"dense\"],\n    bias=\"none\",\n    lora_dropout=0.05,\n    task_type=\"CAUSAL_LM\",\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-18T19:47:47.672097Z","iopub.execute_input":"2025-02-18T19:47:47.672462Z","iopub.status.idle":"2025-02-18T19:47:47.676453Z","shell.execute_reply.started":"2025-02-18T19:47:47.672430Z","shell.execute_reply":"2025-02-18T19:47:47.675658Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"\n# ‚úÖ Enable Gradient Checkpointing\nmodel.gradient_checkpointing_enable()\n\n# ‚úÖ Prepare model for LoRA\nmodel = prepare_model_for_kbit_training(model)\nmodel.config.use_cache = False  \n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-18T19:48:00.928021Z","iopub.execute_input":"2025-02-18T19:48:00.928314Z","iopub.status.idle":"2025-02-18T19:48:00.951974Z","shell.execute_reply.started":"2025-02-18T19:48:00.928290Z","shell.execute_reply":"2025-02-18T19:48:00.951350Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"# ‚úÖ Apply LoRA Adapter\npeft_model = get_peft_model(model, config)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-18T19:48:17.408863Z","iopub.execute_input":"2025-02-18T19:48:17.409149Z","iopub.status.idle":"2025-02-18T19:48:17.525834Z","shell.execute_reply.started":"2025-02-18T19:48:17.409127Z","shell.execute_reply":"2025-02-18T19:48:17.525162Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"\n# ‚úÖ Preprocessing Function (Move Data to GPU)\ndef preprocess_function(examples):\n    model_inputs = tokenizer(\n        examples[\"dialogue\"], \n        padding=\"max_length\", \n        truncation=True, \n        max_length=512\n    )\n\n    labels = tokenizer(\n        examples[\"summary\"], \n        padding=\"max_length\", \n        truncation=True, \n        max_length=128\n    )\n\n    model_inputs[\"labels\"] = labels[\"input_ids\"]\n    model_inputs[\"labels\"] = [\n        [-100 if token == tokenizer.pad_token_id else token for token in label]\n        for label in model_inputs[\"labels\"]\n    ]\n\n    # ‚úÖ Move to GPU\n    model_inputs = {k: torch.tensor(v).to(device) for k, v in model_inputs.items()}\n    return model_inputs\n\n# ‚úÖ Apply preprocessing to dataset\ntrain_dataset = dataset[\"train\"].map(preprocess_function, batched=True)\neval_dataset = dataset[\"validation\"].map(preprocess_function, batched=True)\n\n# ‚úÖ Data Collator\ndata_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-18T19:48:53.514086Z","iopub.execute_input":"2025-02-18T19:48:53.514532Z","iopub.status.idle":"2025-02-18T19:48:56.269210Z","shell.execute_reply.started":"2025-02-18T19:48:53.514504Z","shell.execute_reply":"2025-02-18T19:48:56.268560Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/1999 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"01ce7502166740afa90ff74a3988176b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/499 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"29efcedc79e44dcbacf7f0e063300145"}},"metadata":{}}],"execution_count":16},{"cell_type":"code","source":"#Zero short traing \ndef generate_summary(model, tokenizer, dialogue, max_tokens=100, temperature=0.7):\n    \"\"\"Generate a summary for a given dialogue using the base model (before fine-tuning).\"\"\"\n\n    # ‚úÖ Tokenize input and move to GPU if available\n    inputs = tokenizer(dialogue, return_tensors=\"pt\", truncation=True, max_length=512).to(device)\n\n    # ‚úÖ Generate summary using the base model\n    output = model.generate(\n        **inputs, \n        max_new_tokens=max_tokens, \n        do_sample=True,  # Enables randomness for diverse outputs\n        temperature=temperature,  # Adjusts creativity (lower = deterministic, higher = diverse)\n        num_beams=5,  # Beam search for better quality\n        early_stopping=True\n    )\n\n    # ‚úÖ Decode and return the generated summary\n    return tokenizer.decode(output[0], skip_special_tokens=True)\n\n# ‚úÖ Sample Dialogue (Replace with real test cases)\nsample_dialogue = \"\"\"\nA: Hey, how was your day?\nB: It was good! I had a lot of meetings, but I also managed to finish my report.\nA: That sounds productive! Anything exciting happen?\nB: Not really, just the usual work stuff.\n\"\"\"\n\n# ‚úÖ Generate Summary (Zero-Shot)\nbase_summary = generate_summary(model, tokenizer, sample_dialogue)\n\nprint(\"\\nüîπ **Base Model (Zero-Shot) Summary:**\", base_summary)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-18T19:55:43.096426Z","iopub.execute_input":"2025-02-18T19:55:43.096785Z","iopub.status.idle":"2025-02-18T19:55:50.444839Z","shell.execute_reply.started":"2025-02-18T19:55:43.096754Z","shell.execute_reply":"2025-02-18T19:55:50.443977Z"}},"outputs":[{"name":"stdout","text":"\nüîπ **Base Model (Zero-Shot) Summary:** \nA: Hey, how was your day?\nB: It was good! I had a lot of meetings, but I also managed to finish my report.\nA: That sounds productive! Anything exciting happen?\nB: Not really, just the usual work stuff.\nA: That's okay, it's good to have a routine. How about you?\nB: Same old, same old. I've been working on a new project, but it's still in the early stages.\nA: That's good to hear. What do you think about the new project?\nB: It's interesting, but I'm not sure if it's going to be successful.\nA: That's a good point\n","output_type":"stream"}],"execution_count":20},{"cell_type":"code","source":"\n# ‚úÖ Training Arguments\npeft_training_args = TrainingArguments(\n    output_dir=\"./peft-dialogue-summary-training/final-checkpoint\",\n    per_device_train_batch_size=1,  # ‚úÖ Reduce batch size\n    gradient_accumulation_steps=8,  # ‚úÖ Reduce memory\n    max_steps=500,\n    learning_rate=2e-4,\n    optim=\"paged_adamw_8bit\",\n    logging_steps=50,\n    save_strategy=\"steps\",\n    save_steps=100,\n    evaluation_strategy=\"steps\",\n    eval_steps=50,\n    do_eval=True,\n    gradient_checkpointing=True,  # ‚úÖ Save memory\n    fp16=True,  # ‚úÖ Use mixed precision\n    report_to=\"none\",\n    overwrite_output_dir=True,\n    group_by_length=True,\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-18T19:57:18.882042Z","iopub.execute_input":"2025-02-18T19:57:18.882444Z","iopub.status.idle":"2025-02-18T19:57:18.913607Z","shell.execute_reply.started":"2025-02-18T19:57:18.882407Z","shell.execute_reply":"2025-02-18T19:57:18.912703Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead\n  warnings.warn(\n","output_type":"stream"}],"execution_count":21},{"cell_type":"code","source":"# ‚úÖ Trainer\npeft_trainer = Trainer(\n    model=peft_model,\n    train_dataset=train_dataset,\n    eval_dataset=eval_dataset,\n    args=peft_training_args,\n    data_collator=data_collator,\n)\n\n# ‚úÖ Free memory before training\ntorch.cuda.empty_cache()\n\n# ‚úÖ Train Model\npeft_trainer.train()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-18T19:57:35.017275Z","iopub.execute_input":"2025-02-18T19:57:35.017665Z","iopub.status.idle":"2025-02-18T21:02:26.354316Z","shell.execute_reply.started":"2025-02-18T19:57:35.017631Z","shell.execute_reply":"2025-02-18T21:02:26.353651Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='500' max='500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [500/500 1:04:43, Epoch 2/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>50</td>\n      <td>14.051700</td>\n      <td>1.688373</td>\n    </tr>\n    <tr>\n      <td>100</td>\n      <td>13.123300</td>\n      <td>1.653714</td>\n    </tr>\n    <tr>\n      <td>150</td>\n      <td>13.237200</td>\n      <td>1.644818</td>\n    </tr>\n    <tr>\n      <td>200</td>\n      <td>12.773700</td>\n      <td>1.648048</td>\n    </tr>\n    <tr>\n      <td>250</td>\n      <td>12.779600</td>\n      <td>1.642578</td>\n    </tr>\n    <tr>\n      <td>300</td>\n      <td>12.736400</td>\n      <td>1.640812</td>\n    </tr>\n    <tr>\n      <td>350</td>\n      <td>12.976200</td>\n      <td>1.636319</td>\n    </tr>\n    <tr>\n      <td>400</td>\n      <td>12.709100</td>\n      <td>1.637676</td>\n    </tr>\n    <tr>\n      <td>450</td>\n      <td>12.576900</td>\n      <td>1.637453</td>\n    </tr>\n    <tr>\n      <td>500</td>\n      <td>12.682800</td>\n      <td>1.633713</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n","output_type":"stream"},{"execution_count":22,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=500, training_loss=12.96468603515625, metrics={'train_runtime': 3890.0002, 'train_samples_per_second': 1.028, 'train_steps_per_second': 0.129, 'total_flos': 1.2743361379172352e+16, 'train_loss': 12.96468603515625, 'epoch': 2.0})"},"metadata":{}}],"execution_count":22},{"cell_type":"code","source":"import torch\nimport os\n\n# ‚úÖ Define save directory\nsave_directory = \"/kaggle/working/peft_lora_finetuned_model\"\n\n# ‚úÖ Ensure the directory exists\nos.makedirs(save_directory, exist_ok=True)\n\n# ‚úÖ Save tokenizer\ntokenizer.save_pretrained(save_directory)\n\n# ‚úÖ Save PEFT LoRA Model\npeft_model.save_pretrained(save_directory)\n\nprint(f\"Model saved to {save_directory}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-18T21:08:33.074236Z","iopub.execute_input":"2025-02-18T21:08:33.074585Z","iopub.status.idle":"2025-02-18T21:08:33.559431Z","shell.execute_reply.started":"2025-02-18T21:08:33.074520Z","shell.execute_reply":"2025-02-18T21:08:33.558514Z"}},"outputs":[{"name":"stdout","text":"Model saved to /kaggle/working/peft_lora_finetuned_model\n","output_type":"stream"}],"execution_count":23},{"cell_type":"code","source":"def generate_summary(model, tokenizer, dialogue, max_tokens=100):\n    \"\"\"Generate summary for a given dialogue using the fine-tuned model.\"\"\"\n    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n    \n    inputs = tokenizer(dialogue, return_tensors=\"pt\", truncation=True, max_length=512).to(device)\n    output = model.generate(**inputs, max_new_tokens=max_tokens, do_sample=True, temperature=0.7)\n    \n    return tokenizer.decode(output[0], skip_special_tokens=True)\n\n\nsample_dialogue = \"\"\"\nA: Hey, how was your day?\nB: It was good! I had a lot of meetings, but I also managed to finish my report.\nA: That sounds productive! Anything exciting happen?\nB: Not really, just the usual work stuff.\n\"\"\"\n\n# Generate Summary\nsummary = generate_summary(peft_model, tokenizer, sample_dialogue)\nprint(\"Generated Summary:\", summary)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-18T21:08:37.055940Z","iopub.execute_input":"2025-02-18T21:08:37.056226Z","iopub.status.idle":"2025-02-18T21:08:41.028217Z","shell.execute_reply.started":"2025-02-18T21:08:37.056205Z","shell.execute_reply":"2025-02-18T21:08:41.027486Z"}},"outputs":[{"name":"stdout","text":"Generated Summary: \nA: Hey, how was your day?\nB: It was good! I had a lot of meetings, but I also managed to finish my report.\nA: That sounds productive! Anything exciting happen?\nB: Not really, just the usual work stuff.\nA: I'm glad you made it through the day.\nB: Yeah, it was pretty long.\nA: Did you take any breaks?\nB: No, I was really busy.\nA: I guess that's my luck.\nB: I don't know, I just can't seem to calm down.\nA: That's too bad. Did you have a good lunch?\nB: I had a sandwich at work.\n","output_type":"stream"}],"execution_count":24},{"cell_type":"code","source":"import torch\nimport evaluate\n\n# ‚úÖ Load ROUGE metric\nrouge = evaluate.load(\"rouge\")\n\ndef compute_rouge_scores(model, tokenizer, dataset, num_samples=100, max_length=128):\n    \"\"\"\n    Computes ROUGE scores for the model on a given dataset.\n    \"\"\"\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n    generated_summaries = []\n    reference_summaries = []\n\n    # ‚úÖ Select `num_samples` from the dataset safely\n    num_samples = min(num_samples, len(dataset))  # Prevent out-of-bounds errors\n    dataset_sample = dataset.select(range(num_samples))\n\n    model.eval()  # ‚úÖ Set model to evaluation mode\n\n    # ‚úÖ Generate predictions\n    for example in dataset_sample:\n        dialogue = example[\"dialogue\"]\n        reference_summary = example[\"summary\"]\n\n        # ‚úÖ Tokenize input and move to device\n        inputs = tokenizer(dialogue, return_tensors=\"pt\", truncation=True, padding=True, max_length=512).to(device)\n\n        # ‚úÖ Generate summary (ensure safe LoRA inference)\n        with torch.no_grad():\n            output = model.generate(\n                **inputs,\n                max_new_tokens=max_length,\n                do_sample=True,\n                temperature=0.7\n            )\n\n        # ‚úÖ Decode output and store results\n        generated_summary = tokenizer.decode(output[0], skip_special_tokens=True)\n        generated_summaries.append(generated_summary)\n        reference_summaries.append(reference_summary)\n\n    # ‚úÖ Compute ROUGE scores\n    results = rouge.compute(predictions=generated_summaries, references=reference_summaries)\n\n    return results\n\n# ‚úÖ Run ROUGE Evaluation on the Test Set\nrouge_scores = compute_rouge_scores(peft_model, tokenizer, eval_dataset, num_samples=100)\nprint(\"ROUGE Scores:\", rouge_scores)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-18T21:31:14.391652Z","iopub.execute_input":"2025-02-18T21:31:14.391974Z","iopub.status.idle":"2025-02-18T21:39:41.000622Z","shell.execute_reply.started":"2025-02-18T21:31:14.391947Z","shell.execute_reply":"2025-02-18T21:39:40.999694Z"}},"outputs":[{"name":"stdout","text":"ROUGE Scores: {'rouge1': 0.13809146286245066, 'rouge2': 0.04633682178486702, 'rougeL': 0.10796824646248565, 'rougeLsum': 0.12151004807384524}\n","output_type":"stream"}],"execution_count":38},{"cell_type":"code","source":"import torch\nimport os\nfrom huggingface_hub import HfApi, HfFolder\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nfrom peft import PeftModel\n\n# ‚úÖ Step 1: Define Hugging Face API Token \nhf_token = \"API Tokken\"  # \n\n# ‚úÖ Step 2: Save the token for authentication\nHfFolder.save_token(hf_token)\n\n# ‚úÖ Step 3: Define Repository Name (Replace with your HF username & model name)\nrepo_name = \"username/TinyLlama-1.1B-finetuned-peft\"\n\n# ‚úÖ Step 4: Define Save Directory in Kaggle\nsave_directory = \"/kaggle/working/peft_lora_finetuned_model\"\nos.makedirs(save_directory, exist_ok=True)\n\n# ‚úÖ Step 5: Save Tokenizer & Model to Disk\ntokenizer.save_pretrained(save_directory)\npeft_model.save_pretrained(save_directory)\n\nprint(f\"Model saved locally in {save_directory}\")\n\n# ‚úÖ Step 6: Upload Model & Tokenizer to Hugging Face\npeft_model.push_to_hub(repo_name, use_auth_token=hf_token)\ntokenizer.push_to_hub(repo_name, use_auth_token=hf_token)\n\nprint(f\"üöÄ Model successfully uploaded to: https://huggingface.co/{repo_name}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-18T21:20:02.237521Z","iopub.execute_input":"2025-02-18T21:20:02.237849Z","iopub.status.idle":"2025-02-18T21:20:07.890200Z","shell.execute_reply.started":"2025-02-18T21:20:02.237826Z","shell.execute_reply":"2025-02-18T21:20:07.889434Z"}},"outputs":[{"name":"stdout","text":"Model saved locally in /kaggle/working/peft_lora_finetuned_model\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"adapter_model.safetensors:   0%|          | 0.00/12.3M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"365f25d9331f4714a4fba96e04b0fb25"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/5.17k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d5de7f2ea23e4350b78c2a9429f05b86"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"01755f34744d4096a0e713aeb268c535"}},"metadata":{}},{"name":"stdout","text":"üöÄ Model successfully uploaded to: https://huggingface.co/Ishan152/TinyLlama-1.1B-finetuned-peft\n","output_type":"stream"}],"execution_count":31}]}