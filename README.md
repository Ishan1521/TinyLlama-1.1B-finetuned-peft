# TinyLlama-1.1B-finetuned-peft
# ðŸš€ LoRA-TinyLlama for Dialogue Summarization

This repository contains code for **fine-tuning TinyLlama (1.1B) with LoRA (Low-Rank Adaptation)** on the **DialogSum dataset** for **dialogue summarization**. The model is quantized using **4-bit precision (BitsAndBytes)** to reduce memory usage, making it efficient for training and inference.

## ðŸ“Œ Features
- âœ… **TinyLlama-1.1B** fine-tuned using **LoRA** (Low-Rank Adaptation).
- âœ… **4-bit quantization** (using `BitsAndBytes`) for efficient memory usage.
- âœ… **DialogSum dataset** used for training.
- âœ… **Gradient checkpointing & mixed precision** for optimized training.
- âœ… **ROUGE metric evaluation** for automated performance assessment.

---

## âš¡ Installation

### **1 Run in kaggle notebook 
```bash

 Use Kaggle notebook to run the script, and make sure to turn on internet and GPU accelerator in setting.







